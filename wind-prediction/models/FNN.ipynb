{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mbhavye-mathur\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "from WindModel import *\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "WindDataset.init(0.1)\n",
    "\n",
    "train = WindDataset(\"train\")\n",
    "validation = WindDataset(\"validation\")\n",
    "test = WindDataset(\"test\")\n",
    "\n",
    "del WindDataset.data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "INPUT_SIZE = 15\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "LOSS_FUNC = torch.nn.MSELoss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_dense_model(input_size: int,\n",
    "                    hidden_sizes: list[int],\n",
    "                    output_size: int,\n",
    "                    activation_func: callable):\n",
    "    layers = []\n",
    "\n",
    "    for size in hidden_sizes:\n",
    "        layers.append(torch.nn.Linear(input_size, size))\n",
    "        layers.append(activation_func())\n",
    "        input_size = size\n",
    "\n",
    "    layers.append(torch.nn.Linear(input_size, output_size))\n",
    "\n",
    "    return torch.nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(model, epoch, dl):\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dl:\n",
    "            prediction = model(inputs).squeeze()\n",
    "\n",
    "            mse += torch.nn.functional.mse_loss(prediction, targets)\n",
    "            mae += torch.nn.functional.l1_loss(prediction, targets)\n",
    "\n",
    "    n = len(dl)\n",
    "    wandb.log({\"val_rmse\": ((mse / n) ** 0.5) * stds[VARIABLE]})\n",
    "    wandb.log({\"val_mae\": (mae / n) * stds[VARIABLE]})\n",
    "\n",
    "\n",
    "def train_one_batch(model, optimizer, criterion, batch, batch_idx):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs, targets = batch\n",
    "\n",
    "    prediction = model(inputs).squeeze()\n",
    "    loss = criterion(prediction, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch_idx % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            rmse = (torch.nn.functional.mse_loss(prediction, targets) ** 0.5) * stds[VARIABLE]\n",
    "            wandb.log({\"train_loss\": loss})\n",
    "            wandb.log({\"train_rmse\": rmse})\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, criterion, epoch, dl):\n",
    "    data = iter(dl)\n",
    "    for i in range(len(dl)):\n",
    "        train_one_batch(model, optimizer, criterion, next(data), i)\n",
    "\n",
    "\n",
    "def main(config={}):\n",
    "    if config[\"lr_scheduler\"] is None and \"lr_scheduler_kwargs\" in config:\n",
    "        config.pop(\"lr_scheduler_kwargs\")\n",
    "\n",
    "    wandb.init(project=f\"MERRA2-{VARIABLE}\", dir=\"wandb-local\", config=config)\n",
    "\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "    batch_size = wandb.config.batch_size\n",
    "    layers = wandb.config.layers\n",
    "    epochs = wandb.config.epochs\n",
    "\n",
    "    activation = wandb.config.activation\n",
    "    activation = getattr(torch.nn, activation)\n",
    "\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=10, pin_memory=True)\n",
    "    validation_dl = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=10, pin_memory=True)\n",
    "\n",
    "    model = get_dense_model(INPUT_SIZE, layers, OUTPUT_SIZE, activation)\n",
    "    model = model.to(DEVICE)\n",
    "    print(model)\n",
    "\n",
    "    criterion = LOSS_FUNC()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if (scheduler := wandb.config.lr_scheduler) is None:\n",
    "        scheduler = None\n",
    "    elif scheduler == \"StepLR\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, **wandb.config.lr_scheduler_kwargs, verbose=True)\n",
    "\n",
    "    wandb.watch(model, log_freq=100)\n",
    "    model.train()\n",
    "\n",
    "    for ep in tqdm(range(epochs)):\n",
    "        print(ep, end=\" \")\n",
    "\n",
    "        wandb.log({\"epoch\": ep})\n",
    "\n",
    "        train_one_epoch(model, optimizer, criterion, ep, train_dl)\n",
    "        evaluate_one_epoch(model, ep, validation_dl)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            wandb.log({\"learning_rate\": scheduler.get_last_lr()})\n",
    "\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Path wandb-local/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path wandb-local/wandb/ wasn't writable, using system temp directory\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.10"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/folders/r5/mzzh3rn14lgb2__wr5h7swqm0000gn/T/wandb/run-20230215_223710-zonp9ovq</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/bhavye-mathur/MERRA2-U/runs/zonp9ovq' target=\"_blank\">daring-rose-102</a></strong> to <a href='https://wandb.ai/bhavye-mathur/MERRA2-U' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/bhavye-mathur/MERRA2-U' target=\"_blank\">https://wandb.ai/bhavye-mathur/MERRA2-U</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/bhavye-mathur/MERRA2-U/runs/zonp9ovq' target=\"_blank\">https://wandb.ai/bhavye-mathur/MERRA2-U/runs/zonp9ovq</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=15, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Adjusting learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/150 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "753bfef42f184f74b4432e8ddd1d9445"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Epoch 00000: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhavyemathur/Desktop/Projects/Spherindrical Fourier Transform/MERRA-2/venv3.9/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "2 Epoch 00002: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "3 Epoch 00003: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "4 Epoch 00004: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "5 Epoch 00005: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "6 Epoch 00006: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "7 Epoch 00007: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "8 Epoch 00008: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "9 Epoch 00009: adjusting learning rate of group 0 to 5.0000e-04.\n",
      "10 Epoch 00010: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "11 Epoch 00011: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "12 Epoch 00012: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "13 Epoch 00013: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "14 Epoch 00014: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "15 Epoch 00015: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "16 Epoch 00016: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "17 Epoch 00017: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "18 Epoch 00018: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "19 Epoch 00019: adjusting learning rate of group 0 to 5.0000e-05.\n",
      "20 Epoch 00020: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "21 Epoch 00021: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "22 Epoch 00022: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "23 Epoch 00023: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "24 Epoch 00024: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "25 Epoch 00025: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "26 Epoch 00026: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "27 Epoch 00027: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "28 Epoch 00028: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "29 Epoch 00029: adjusting learning rate of group 0 to 5.0000e-06.\n",
      "30 Epoch 00030: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "31 Epoch 00031: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "32 Epoch 00032: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "33 Epoch 00033: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "34 Epoch 00034: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "35 Epoch 00035: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "36 Epoch 00036: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "37 Epoch 00037: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "38 Epoch 00038: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "39 Epoch 00039: adjusting learning rate of group 0 to 5.0000e-07.\n",
      "40 Epoch 00040: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "41 Epoch 00041: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "42 Epoch 00042: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "43 Epoch 00043: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "44 Epoch 00044: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "45 Epoch 00045: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "46 Epoch 00046: adjusting learning rate of group 0 to 5.0000e-08.\n",
      "47 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m65536\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlearning_rate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0005\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr_scheduler\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mStepLR\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr_scheduler_kwargs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstep_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlayers\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mactivation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReLU\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mestimate_quantile\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mESTIMATE_QUANTILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mDATASET\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m150\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 75\u001B[0m, in \u001B[0;36mmain\u001B[0;34m(config)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ep \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs)):\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28mprint\u001B[39m(ep, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m     \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m     evaluate_one_epoch(model, ep, validation_dl)\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m scheduler:\n",
      "Cell \u001B[0;32mIn[6], line 38\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, optimizer, criterion, epoch, dl)\u001B[0m\n\u001B[1;32m     36\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(dl)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(dl)):\n\u001B[0;32m---> 38\u001B[0m     \u001B[43mtrain_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[6], line 22\u001B[0m, in \u001B[0;36mtrain_one_batch\u001B[0;34m(model, optimizer, criterion, batch, batch_idx)\u001B[0m\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     20\u001B[0m inputs, targets \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m---> 22\u001B[0m prediction \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(prediction, targets)\n\u001B[1;32m     25\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/Projects/Spherindrical Fourier Transform/MERRA-2/venv3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/Spherindrical Fourier Transform/MERRA-2/venv3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/Projects/Spherindrical Fourier Transform/MERRA-2/venv3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Projects/Spherindrical Fourier Transform/MERRA-2/venv3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "main({\n",
    "    \"batch_size\": 65536,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"lr_scheduler\": \"StepLR\",\n",
    "    \"lr_scheduler_kwargs\": {\"step_size\": 25},\n",
    "    \"layers\": [512, 256],\n",
    "    \"activation\": \"PReLU\",\n",
    "    \"estimate_quantile\": ESTIMATE_QUANTILE,\n",
    "    \"dataset\": DATASET,\n",
    "    \"epochs\": 100,\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"name\": f\"sweep-{DATASET}\",\n",
    "    \"metric\": {\n",
    "        \"goal\": \"minimize\",\n",
    "        \"name\": \"val_rmse\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"value\": 65536},\n",
    "        \"learning_rate\": {\"max\": 0.001, \"min\": 0.00005},\n",
    "        \"lr_scheduler\": {\"values\": [None, \"StepLR\"]},\n",
    "        \"lr_scheduler_kwargs\": {\"parameters\": {\"step_size\": {\"max\": 20, \"min\": 10},\n",
    "                                               \"gamma\": {\"max\": 0.75, \"min\": 0.25}}},\n",
    "        \"layers\": {\"values\": [(512, 256), (1024, 512)]},\n",
    "        \"epochs\": {\"value\": 50},\n",
    "        \"activation\": {\"values\": [\"ReLU\", \"PReLU\", \"LeakyReLU\"]},\n",
    "        \"estimate_quantile\": {\"value\": ESTIMATE_QUANTILE},\n",
    "        \"dataset\": {\"value\": DATASET},\n",
    "    },\n",
    "    \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=f\"MERRA2-{VARIABLE}\")\n",
    "wandb.agent(sweep_id, function=main)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/973 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5df4a82250164fdeab70effe75f0b52b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.4102303087711334 m/s\n",
      "MAE:  0.29246410727500916 m/s\n"
     ]
    }
   ],
   "source": [
    "def test(model, dl):\n",
    "    model.eval()\n",
    "\n",
    "    mse = 0\n",
    "    mae = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dl):\n",
    "            prediction = model(inputs).squeeze()\n",
    "\n",
    "            mse += torch.nn.functional.mse_loss(prediction, targets)\n",
    "            mae += torch.nn.functional.l1_loss(prediction, targets)\n",
    "\n",
    "    return (mse / len(dl)) ** 0.5 * stds[VARIABLE], (mae / len(dl)) * stds[VARIABLE]\n",
    "\n",
    "\n",
    "test_dl = DataLoader(test, batch_size=2048, shuffle=False)\n",
    "test_rmse, test_mae = test(model, test)\n",
    "\n",
    "print(f\"RMSE: {test_rmse} m/s\")\n",
    "print(f\"MAE:  {test_mae} m/s\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
